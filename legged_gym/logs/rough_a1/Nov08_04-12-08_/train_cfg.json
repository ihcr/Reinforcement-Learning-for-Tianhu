{"algorithm": {"amp_replay_buffer_size": 1000000, "clip_param": 0.2, "desired_kl": 0.01, "entropy_coef": 0.01, "gamma": 0.99, "lam": 0.95, "learning_rate": 0.001, "max_grad_norm": 1.0, "num_learning_epochs": 5, "num_mini_batches": 4, "schedule": "adaptive", "use_clipped_value_loss": true, "value_loss_coef": 1.0}, "init_member_classes": {}, "policy": {"activation": "elu", "actor_hidden_dims": [512, 256, 128], "critic_hidden_dims": [512, 256, 128], "init_noise_std": 1.0}, "runner": {"algorithm_class_name": "AMPTSPPO", "amp_discr_hidden_dims": [1024, 512], "amp_motion_files": ["datasets/mocap_motions/rightturn0.txt", "datasets/mocap_motions/pace1.txt", "datasets/mocap_motions/pace0.txt", "datasets/mocap_motions/trot1.txt", "datasets/mocap_motions/trot0.txt", "datasets/mocap_motions/leftturn0.txt"], "amp_num_preload_transitions": 600000, "amp_reward_coef": 2.0, "amp_task_reward_lerp": 0.3, "checkpoint": -1, "experiment_name": "rough_a1", "include_history_steps": null, "load_run": "", "max_iterations": 12000, "min_normalized_std": [0.05, 0.02, 0.05, 0.05, 0.02, 0.05, 0.05, 0.02, 0.05, 0.05, 0.02, 0.05], "num_steps_per_env": 24, "policy_class_name": "ActorCriticAmpTs", "resume": false, "resume_path": "legged_gym/logs/rough_a1", "run_name": "", "save_interval": 500}, "runner_class_name": "AMPTSOnPolicyRunner", "seed": 2022}