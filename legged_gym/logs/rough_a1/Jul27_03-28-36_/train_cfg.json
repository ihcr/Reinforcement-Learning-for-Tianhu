{"algorithm": {"clip_param": 0.2, "desired_kl": 0.01, "entropy_coef": 0.01, "gamma": 0.99, "lam": 0.95, "learning_rate": 0.001, "max_grad_norm": 1.0, "num_learning_epochs": 5, "num_mini_batches": 4, "schedule": "adaptive", "use_clipped_value_loss": true, "value_loss_coef": 1.0}, "init_member_classes": {}, "policy": {"activation": "elu", "actor_hidden_dims": [512, 256, 128], "critic_hidden_dims": [512, 256, 128], "init_noise_std": 1.0}, "runner": {"algorithm_class_name": "PPO", "checkpoint": -1, "experiment_name": "rough_a1", "load_run": "", "max_iterations": 600, "num_steps_per_env": 24, "policy_class_name": "ActorCritic", "resume": false, "resume_path": "legged_gym/logs/rough_a1", "run_name": "", "save_interval": 500}, "runner_class_name": "OnPolicyRunner", "seed": 2022}